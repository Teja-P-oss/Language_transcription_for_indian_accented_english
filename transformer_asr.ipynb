{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch3lwM3vQh0l"
   },
   "source": [
    "# Automatic Speech Recognition with Transformer\n",
    "\n",
    "Training a sequence-to-sequence Transformer for automatic speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kf1fWM_rQh0o"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n",
    "ASR can be treated as a sequence-to-sequence problem, where the\n",
    "audio can be represented as a sequence of feature vectors\n",
    "and the text as a sequence of characters, words, or subword tokens.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n",
    "- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n",
    "- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LL6GPZX7Qh0q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tza5GfQAQh0r"
   },
   "source": [
    "## Define the Transformer Input Layer\n",
    "\n",
    "When processing past target tokens for the decoder, we compute the sum of\n",
    "position embeddings and token embeddings.\n",
    "\n",
    "When processing audio features, we apply convolutional layers to downsample\n",
    "them (via convolution stides) and process local relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ioUVHAhYQh0s"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class SpeechFeatureEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        #tf.debugging.check_numerics(x, message='')\n",
    "        x = self.conv1(x)\n",
    "        #tf.debugging.check_numerics(x, message='')\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz5Az2XKQh0u"
   },
   "source": [
    "## Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MHkERWBRQh0v"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxkqeTIDQh0w"
   },
   "source": [
    "## Transformer Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WqSXyA1UQh0x"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TZBQpZoQh0y"
   },
   "source": [
    "## Complete the Transformer model\n",
    "\n",
    "Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n",
    "During training, we give the decoder the target character sequence shifted to the left\n",
    "as input. During inference, the decoder uses its own past predictions to predict the\n",
    "next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ZUVYQ7uBQh0z"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        #tf.debugging.check_numerics(source, message='')\n",
    "        x = self.encoder(source)\n",
    "        y = self.decode(x, target)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        \n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7SoT6IpQh00"
   },
   "source": [
    "## Preprocess the dataset\n",
    "\n",
    "We preprocess dataset to create dictionary with mapping of audio files to corresponding transcripts. Then, we create training and test dataset with this dictionary with train split of 99% and test split of 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ISpiFi7jQh01"
   },
   "outputs": [],
   "source": [
    "id_to_text = {}\n",
    "csv_file = 'mapping.csv'\n",
    "\n",
    "with open(csv_file, encoding=\"utf-8\") as f:\n",
    "    wavs = []\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        id = line.strip().split(\",\")[0]\n",
    "        text = line.strip().split(\",\")[4]\n",
    "        id_to_text[id] = text\n",
    "\n",
    "def get_data(csv_file, prefix_path, id_to_text, maxlen=50):\n",
    "    \"\"\" returns mapping of audio paths and transcription texts \"\"\"\n",
    "    data = []\n",
    "    with open(csv_file, encoding='utf-8') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            id = line.strip().split(\",\")[0]\n",
    "            audio_path = prefix_path + line.strip().split(\",\")[-1]\n",
    "            if len(id_to_text[id]) < maxlen:\n",
    "                data.append({\"audio\": audio_path, \"text\": id_to_text[id]})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qi20I6CmQh02",
    "outputId": "8411817b-39bb-48ed-bcc7-33d49ace1810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab\n",
    "\n",
    "prefix_path = ''\n",
    "max_target_len = 310  # all transcripts in out data are < 310 characters\n",
    "data = get_data(csv_file, prefix_path, id_to_text, max_target_len)\n",
    "vectorizer = VectorizeChar(max_target_len)\n",
    "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "\n",
    "\n",
    "def create_text_ds(data):\n",
    "    texts = [_[\"text\"] for _ in data]\n",
    "    text_ds = [vectorizer(t) for t in texts]\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
    "    return text_ds\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    # spectrogram using stft\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
    "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
    "    # normalisation\n",
    "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
    "    x = (x - means)\n",
    "    x = tf.math.divide_no_nan(x, stddevs)\n",
    "    audio_len = tf.shape(x)[0]\n",
    "    # padding to 2754 seconds\n",
    "    pad_len = 2754\n",
    "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
    "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_audio_ds(data):\n",
    "    flist = [_[\"audio\"] for _ in data]\n",
    "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
    "    audio_ds = audio_ds.map(\n",
    "        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    return audio_ds\n",
    "\n",
    "\n",
    "def create_tf_dataset(data, bs=4):\n",
    "    audio_ds = create_audio_ds(data)\n",
    "    text_ds = create_text_ds(data)\n",
    "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
    "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
    "    ds = ds.batch(bs)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "split = int(100000*0.99)\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]\n",
    "ds = create_tf_dataset(train_data, bs=128)\n",
    "val_ds = create_tf_dataset(test_data, bs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4n3UVxlQh02"
   },
   "source": [
    "## Callbacks to display predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "533N-L6kQh03"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch containing the keys \"source\" and \"target\"\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        source = self.batch[\"source\"]\n",
    "        target = self.batch[\"target\"].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:     {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNgKYiR8Qh03"
   },
   "source": [
    "## Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ztXOZhRxQh03"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_lr=0.00001,\n",
    "        lr_after_warmup=0.001,\n",
    "        final_lr=0.00001,\n",
    "        warmup_epochs=15,\n",
    "        decay_epochs=85,\n",
    "        steps_per_epoch=203,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_after_warmup = lr_after_warmup\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def calculate_lr(self, epoch):\n",
    "        \"\"\" linear warm up - linear decay \"\"\"\n",
    "        warmup_lr = (\n",
    "            self.init_lr\n",
    "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
    "        )\n",
    "        decay_lr = tf.math.maximum(\n",
    "            self.final_lr,\n",
    "            self.lr_after_warmup\n",
    "            - (epoch - self.warmup_epochs)\n",
    "            * (self.lr_after_warmup - self.final_lr)\n",
    "            / (self.decay_epochs),\n",
    "        )\n",
    "        return tf.math.minimum(warmup_lr, decay_lr)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = step // self.steps_per_epoch\n",
    "        return self.calculate_lr(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZmUbiZeQh04"
   },
   "source": [
    "## Create & train the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ky5vYDHp9M47",
    "outputId": "23e95629-8a5f-493f-bd03-e9b81186843e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774/774 [==============================] - 709s 908ms/step - loss: 1.1807 - val_loss: 1.0395\n",
      "Epoch 1/100\n",
      "774/774 [==============================] - 709s 915ms/step - loss: 0.8094 - val_loss: 0.7618\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <in that is is is is is is in this is is is is is is is is in the is is in the is is is>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <the car the conding the conding of the conting to the requal to the requal to the is>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <it is a see this this this this this this this this this this this this a so the so the sign>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <of the constant the so the contion of the contion of the contion the so in the contion>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of the posing the read the intery the read the so the reasing the reading the reading the re>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <the in the require the read this the read the requal to the read the so the requal>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <this is the this the remal be are the restant the required to the contions>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <the in the requal to the and the constion of the consting the consition of the contion>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <the in the consition of the consition of the position the so the requal to the posit>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <and the so the is of in the contion of interant the so the is of interation of the sure>\n",
      "\n",
      "Epoch 2/100\n",
      "774/774 [==============================] - 707s 913ms/step - loss: 0.7124 - val_loss: 0.7035\n",
      "Epoch 3/100\n",
      "774/774 [==============================] - 697s 900ms/step - loss: 0.6505 - val_loss: 0.6544\n",
      "Epoch 4/100\n",
      "774/774 [==============================] - 703s 908ms/step - loss: 0.6099 - val_loss: 0.6263\n",
      "Epoch 5/100\n",
      "774/774 [==============================] - 698s 900ms/step - loss: 0.5801 - val_loss: 0.6056\n",
      "Epoch 6/100\n",
      "774/774 [==============================] - 695s 897ms/step - loss: 0.5568 - val_loss: 0.5906\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting be in the sproblem the sproblem of the sproblem of the sproblem of the sproblowise>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you have large the problem the problem the problem the problem the position the>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <the is and the power this say is a say and the power this and the power this a constion>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <of i is equal to the to the to the to the to the to the to the to let us a such or>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of the so this is invery here faction which is is is variated it is variated it is variate>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <varing a finitely arely neer alinearly you can or itermiste basumes the of i time>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <this is is is is an this is is is is is a limporitel in this is is an this is in this>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and the rective up of the rection it will you network and because ineer to the>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <convector be # or # base to be you convector because to be # one pace to be # this>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so instrop here of signitude interse the or signative intror instrop here>\n",
      "\n",
      "Epoch 7/100\n",
      "774/774 [==============================] - 693s 895ms/step - loss: 0.5379 - val_loss: 0.5781\n",
      "Epoch 8/100\n",
      "774/774 [==============================] - 697s 900ms/step - loss: 0.5218 - val_loss: 0.5683\n",
      "Epoch 9/100\n",
      "774/774 [==============================] - 694s 896ms/step - loss: 0.5080 - val_loss: 0.5605\n",
      "Epoch 10/100\n",
      "774/774 [==============================] - 702s 907ms/step - loss: 0.4961 - val_loss: 0.5557\n",
      "Epoch 11/100\n",
      "774/774 [==============================] - 702s 906ms/step - loss: 0.4944 - val_loss: 0.5663\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting b and it is is a problem this problem this problem the sproblem which>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <we have a large it is the for which is a h comploition and of all r reflice>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <the say is all then the for this say is all than the for this say is all than the hoppleximal>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies generally hold alway hold alright. so the position value hold all you hold>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of a so the interfaction which is varianchity inchise varianchity insuling>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <mading a linearly you are faction>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <that is is the liness is the liness is the lineter of the liness in this is the limize>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and therect as the lineter as the lineter as the lineter and before the>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <convective contraction contractive context to minimize the correct to be rone this>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so instrop here signal signal so instrop here are>\n",
      "\n",
      "Epoch 12/100\n",
      "774/774 [==============================] - 697s 899ms/step - loss: 0.4887 - val_loss: 0.5573\n",
      "Epoch 13/100\n",
      "774/774 [==============================] - 691s 892ms/step - loss: 0.4875 - val_loss: 0.5604\n",
      "Epoch 14/100\n",
      "774/774 [==============================] - 693s 895ms/step - loss: 0.4835 - val_loss: 0.5527\n",
      "Epoch 15/100\n",
      "774/774 [==============================] - 695s 897ms/step - loss: 0.4835 - val_loss: 0.5547\n",
      "Epoch 16/100\n",
      "774/774 [==============================] - 699s 903ms/step - loss: 0.4755 - val_loss: 0.5537\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting b and it is in the strobarial>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you have allogenously than the home module in the hompose>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <the same this samply the hump what this say enother>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <a plies equal to the post the post the positional>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <so we arefaction which is varianially you are faction>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <mading and alinear>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <greater is the linear formulant this the linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and again or the linear product as as a cost of the linety for the>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <practive convenies the correct to convector convent.>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so signal cost of here signal cost of here>\n",
      "\n",
      "Epoch 17/100\n",
      "774/774 [==============================] - 688s 888ms/step - loss: 0.4683 - val_loss: 0.5520\n",
      "Epoch 18/100\n",
      "774/774 [==============================] - 694s 896ms/step - loss: 0.4754 - val_loss: 0.5685\n",
      "Epoch 19/100\n",
      "774/774 [==============================] - 689s 889ms/step - loss: 0.4703 - val_loss: 0.5581\n",
      "Epoch 20/100\n",
      "774/774 [==============================] - 692s 894ms/step - loss: 0.4627 - val_loss: 0.5550\n",
      "Epoch 21/100\n",
      "774/774 [==============================] - 694s 896ms/step - loss: 0.4578 - val_loss: 0.5523\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <impression b and if the strong we and if the strong you are>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you have alloging the function is the form which>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <the same is also the same is also the same is also the same>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the proto lectrice>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <so the so we are faction alinear faction>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <maning allinear faction>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <which is is a linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and there can be for the linear transpose or cost of the>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that impoint the convector than>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <how instrop here or signal transpose>\n",
      "\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774/774 [==============================] - 694s 895ms/step - loss: 0.4536 - val_loss: 0.5569\n",
      "Epoch 23/100\n",
      "774/774 [==============================] - 690s 891ms/step - loss: 0.4877 - val_loss: 0.6005\n",
      "Epoch 24/100\n",
      "774/774 [==============================] - 695s 897ms/step - loss: 0.4784 - val_loss: 0.5744\n",
      "Epoch 25/100\n",
      "774/774 [==============================] - 708s 914ms/step - loss: 0.4723 - val_loss: 0.5728\n",
      "Epoch 26/100\n",
      "774/774 [==============================] - 710s 916ms/step - loss: 0.4630 - val_loss: 0.5538\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <impressing the sproblem of the sting b and if you see the practical let of the>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <have allowed>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <the same this same this same this same this same this same this>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <supplies equal to the problem equal>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <so we cossion which is maying the linear>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <mating allinear faction>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <which is the linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <change or cost of the linear program>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that you can ok. so this is the correct to minimize the correct to minimize>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <how instop here or signal>\n",
      "\n",
      "Epoch 27/100\n",
      "774/774 [==============================] - 696s 898ms/step - loss: 0.4698 - val_loss: 0.5878\n",
      "Epoch 28/100\n",
      "774/774 [==============================] - 699s 903ms/step - loss: 0.4667 - val_loss: 0.5531\n",
      "Epoch 29/100\n",
      "774/774 [==============================] - 693s 894ms/step - loss: 0.4552 - val_loss: 0.5496\n",
      "Epoch 30/100\n",
      "774/774 [==============================] - 695s 897ms/step - loss: 0.4495 - val_loss: 0.6060\n",
      "Epoch 31/100\n",
      "774/774 [==============================] - 700s 904ms/step - loss: 0.4549 - val_loss: 0.5454\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <implies see the practical net of this problem you can it is a constrain>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you have a large if you network the or there is a compleximate>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <the say the say the say the say the same is all the total>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the post the pose next general you hold you hold>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <so the program>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <many inerfaction allinear faction>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <with is is a linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <again or consider can be formulate as a linear the linear the linear the linear to>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that you can point to minimize the connect.>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <significally starter significant the significan>\n",
      "\n",
      "Epoch 32/100\n",
      "774/774 [==============================] - 698s 902ms/step - loss: 0.4439 - val_loss: 0.5312\n",
      "Epoch 33/100\n",
      "774/774 [==============================] - 693s 895ms/step - loss: 0.4389 - val_loss: 0.5328\n",
      "Epoch 34/100\n",
      "774/774 [==============================] - 690s 891ms/step - loss: 0.4347 - val_loss: 0.5291\n",
      "Epoch 35/100\n",
      "774/774 [==============================] - 690s 891ms/step - loss: 0.4315 - val_loss: 0.5246\n",
      "Epoch 36/100\n",
      "774/774 [==============================] - 696s 899ms/step - loss: 0.4282 - val_loss: 0.5200\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting b # this problem in the practical need this problem of this problem>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <we have a large if you can the comit transpose>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <ten the functional>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the positive magnitude the positive magnitude the positing general>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <prographically interfaction>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <mating allinear faction>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <which is the linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <h const of the formulated as the linear program>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that you can point transform than>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so which tracket constrop here>\n",
      "\n",
      "Epoch 37/100\n",
      "774/774 [==============================] - 691s 892ms/step - loss: 0.4251 - val_loss: 0.5174\n",
      "Epoch 38/100\n",
      "774/774 [==============================] - 693s 895ms/step - loss: 0.4222 - val_loss: 0.5120\n",
      "Epoch 39/100\n",
      "774/774 [==============================] - 691s 892ms/step - loss: 0.4197 - val_loss: 0.5067\n",
      "Epoch 40/100\n",
      "774/774 [==============================] - 699s 902ms/step - loss: 0.4172 - val_loss: 0.5049\n",
      "Epoch 41/100\n",
      "774/774 [==============================] - 692s 893ms/step - loss: 0.4149 - val_loss: 0.5034\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting b and you can practical nates of this problem>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you have a largefulition at or which is network that is>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <ten of all the says is as the total the says is as the total>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the proto lext stend the proto let>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <prograph>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <mating a linear fashion all inear fashion>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <which number of itel in the linear formulant is a linear formulange>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <h const of a cost of a cost of a constability and be for the linear program>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that you can online. so this>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <all instop here significant has track has top here>\n",
      "\n",
      "Epoch 42/100\n",
      "774/774 [==============================] - 697s 900ms/step - loss: 0.4126 - val_loss: 0.5019\n",
      "Epoch 43/100\n",
      "774/774 [==============================] - 693s 894ms/step - loss: 0.4101 - val_loss: 0.4990\n",
      "Epoch 44/100\n",
      "774/774 [==============================] - 696s 898ms/step - loss: 0.4080 - val_loss: 0.4958\n",
      "Epoch 45/100\n",
      "774/774 [==============================] - 696s 899ms/step - loss: 0.4058 - val_loss: 0.4938\n",
      "Epoch 46/100\n",
      "774/774 [==============================] - 696s 899ms/step - loss: 0.4036 - val_loss: 0.4917\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting b and you can see the practical matrical mater>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you have a largefulition at or which is network this>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <ten of all the says is as the total extend of all the says is as is as is as is as is>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the potical equal to the post lectures demaning generally hold all>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of a fashion which is verying a fashion which is verying alling>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <mating a linear fashion>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <which number of i term is in the linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <a or constabiling and be formulated of the linear the linear program>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that you convector on it is with convector on it is with connect on it is>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <or signalized signal>\n",
      "\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774/774 [==============================] - 711s 918ms/step - loss: 0.4014 - val_loss: 0.4877\n",
      "Epoch 48/100\n",
      "774/774 [==============================] - 713s 920ms/step - loss: 0.3994 - val_loss: 0.4833\n",
      "Epoch 49/100\n",
      "774/774 [==============================] - 692s 894ms/step - loss: 0.3973 - val_loss: 0.4775\n",
      "Epoch 50/100\n",
      "774/774 [==============================] - 695s 897ms/step - loss: 0.3954 - val_loss: 0.4769\n",
      "Epoch 51/100\n",
      "774/774 [==============================] - 695s 897ms/step - loss: 0.3934 - val_loss: 0.4751\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting b for this problem is the processing b for this problem>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you have a large if you can of alrithm which is network this>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <ten the hundred of always the totar lecturnal>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to to the proto lextend value hold lext stange alright>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of # which is verying a fashion which is verying a fashion which is verying all right>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <varying a linear fashion all right which number of i terms if inum>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <which number of i telling a formulate this is the linear formulate this is the linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <change or constability a constability a constability a constability a constabiling a program>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that you minimize the cost of the also so startive conity on x # on it is this>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <alright so will stop here signity in paraction conduction in paraction can paractical>\n",
      "\n",
      "Epoch 52/100\n",
      "774/774 [==============================] - 697s 899ms/step - loss: 0.3914 - val_loss: 0.4707\n",
      "Epoch 53/100\n",
      "774/774 [==============================] - 710s 917ms/step - loss: 0.3893 - val_loss: 0.4676\n",
      "Epoch 54/100\n",
      "774/774 [==============================] - 701s 905ms/step - loss: 0.3873 - val_loss: 0.4659\n",
      "Epoch 55/100\n",
      "774/774 [==============================] - 693s 895ms/step - loss: 0.3855 - val_loss: 0.4631\n",
      "Epoch 56/100\n",
      "774/774 [==============================] - 700s 904ms/step - loss: 0.3838 - val_loss: 0.4601\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting b # and you can see the procement of this problem you can see the problem>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you have a large is network this which is network this>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <tell tend of also the total explains of also the total explains>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <apply is equal to the proto lext stange demaning general demaning general demaning general>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <or forgeter verying a fashion which is varing all indian aling all right>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <varing linear fashion all right which number of item>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <which number of itel given formulate this is a linear formulate this is the linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <can be formedicitnal program difform the linear program difform the linear program>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that you conically to minimize the cost to minimize the cost to minimize the cost this>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <alright so we instop here significal constraint so we instop here>\n",
      "\n",
      "Epoch 57/100\n",
      "774/774 [==============================] - 709s 916ms/step - loss: 0.3823 - val_loss: 0.4582\n",
      "Epoch 58/100\n",
      "774/774 [==============================] - 702s 906ms/step - loss: 0.3807 - val_loss: 0.4571\n",
      "Epoch 59/100\n",
      "774/774 [==============================] - 699s 902ms/step - loss: 0.3792 - val_loss: 0.4568\n",
      "Epoch 60/100\n",
      "774/774 [==============================] - 696s 899ms/step - loss: 0.3775 - val_loss: 0.4551\n",
      "Epoch 61/100\n",
      "774/774 [==============================] - 698s 901ms/step - loss: 0.3759 - val_loss: 0.4534\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting b and it will see the procement of this problem>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you can have a large which is which is which is which is which is which is which is>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <kind of an the of so what this says is also the total external>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the protal explain radived alright>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of a fashion which is varying a fashion which is varying all right>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <varing a linear fashion all right which the number of i tells>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <which number of i tells given formulate this is a linear program the formulation formulation>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <constraint a constability a constability a constability a convery convergent to them>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <factive with a nector with a network alright. so this>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <all encror significal the the significal the the significal the the significal>\n",
      "\n",
      "Epoch 62/100\n",
      "774/774 [==============================] - 699s 903ms/step - loss: 0.3746 - val_loss: 0.4515\n",
      "Epoch 63/100\n",
      "774/774 [==============================] - 700s 904ms/step - loss: 0.3732 - val_loss: 0.4495\n",
      "Epoch 64/100\n",
      "774/774 [==============================] - 698s 902ms/step - loss: 0.3719 - val_loss: 0.4481\n",
      "Epoch 65/100\n",
      "774/774 [==============================] - 699s 902ms/step - loss: 0.3707 - val_loss: 0.4478\n",
      "Epoch 66/100\n",
      "774/774 [==============================] - 698s 901ms/step - loss: 0.3698 - val_loss: 0.4625\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting letter of this processingly and each of this probility and each of this problem>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <we can have a large is network this which is network this which network this>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <ten ten of hunds what this is also the total explain of also what this is also the total>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the post lext rest deming generally hold all you hold all right>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <a factions of # which is bading a factions all link which is bading a factions>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <varing hellinear fashion all right within all right within all right with number of iterms>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <within number of i terms given formulate this is a linear program in formulate this is and>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and or cost of a distributed as a linear process but it as a linear program for but the>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <cofically one x dominimize the cos the cost to minimize the cost to minimize the cost this>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so instop here significal then the stop here>\n",
      "\n",
      "Epoch 67/100\n",
      "774/774 [==============================] - 713s 921ms/step - loss: 0.3744 - val_loss: 0.4420\n",
      "Epoch 68/100\n",
      "774/774 [==============================] - 706s 911ms/step - loss: 0.3697 - val_loss: 0.4401\n",
      "Epoch 69/100\n",
      "774/774 [==============================] - 702s 906ms/step - loss: 0.3676 - val_loss: 0.4398\n",
      "Epoch 70/100\n",
      "774/774 [==============================] - 700s 904ms/step - loss: 0.3661 - val_loss: 0.4391\n",
      "Epoch 71/100\n",
      "774/774 [==============================] - 702s 906ms/step - loss: 0.3646 - val_loss: 0.4383\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interesting leady this problem and it is a practial mater>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you can have a large is which network this which network this>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <tell tell then of ubserve to this says is is a is a longers.>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the total external you hold all you hold all right>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of the forth of # which is manying a fashion link alright.>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <maining a linear fashion all right within all right within all right.>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <within our of i tellinear program i this a linear program the formulate this is a linear>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and a constabling a programmitted earlinear programmitries we should network>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <practive context to minimize the cost associated with a network on text to minimize>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so instop here or significal contral gone execution called on for a significal>\n",
      "\n",
      "Epoch 72/100\n",
      "774/774 [==============================] - 718s 927ms/step - loss: 0.3632 - val_loss: 0.4370\n",
      "Epoch 73/100\n",
      "774/774 [==============================] - 698s 901ms/step - loss: 0.3620 - val_loss: 0.4365\n",
      "Epoch 74/100\n",
      "774/774 [==============================] - 713s 920ms/step - loss: 0.3610 - val_loss: 0.4354\n",
      "Epoch 75/100\n",
      "774/774 [==============================] - 709s 915ms/step - loss: 0.3598 - val_loss: 0.4345\n",
      "Epoch 76/100\n",
      "774/774 [==============================] - 705s 910ms/step - loss: 0.3589 - val_loss: 0.4344\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interestingly and you can see the prional mater of this problem you are>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you can have a large which network this we have a large what this>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <the kind of absorred to this says is alongest of oubstitute this is alongest of obse>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the probal exchange all you hold all you hold all right>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of the for # which is manying a faction which is manying a faction alright>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <maining a fashion all right with number of item.>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <within number of itel given formulate this is a linear program in the formulate this is a line>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <than or cost a many formbinated eas a linear programmitted each of a distot a constable>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <practive context to minimize the cost associated with a nector with a nector with a necond>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so instop so will stop here are a significal contral contral the length of here>\n",
      "\n",
      "Epoch 77/100\n",
      "774/774 [==============================] - 706s 911ms/step - loss: 0.3580 - val_loss: 0.4339\n",
      "Epoch 78/100\n",
      "774/774 [==============================] - 699s 903ms/step - loss: 0.3572 - val_loss: 0.4324\n",
      "Epoch 79/100\n",
      "774/774 [==============================] - 703s 907ms/step - loss: 0.3564 - val_loss: 0.4319\n",
      "Epoch 80/100\n",
      "774/774 [==============================] - 713s 921ms/step - loss: 0.3556 - val_loss: 0.4317\n",
      "Epoch 81/100\n",
      "774/774 [==============================] - 703s 908ms/step - loss: 0.3549 - val_loss: 0.4305\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interestingly and you can see the prictical nature>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you can have a large is we show hundred ten of ubserve it tend of ubserve it ten of ubse>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <the total external of the says is alonger to the same is is alonger to to external>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the total external you hold all you hold all right>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of the forgeter varing a linear fashion all right.>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <varing a linear fashion all right within all right within all right.>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <within number of iteling a program in a formulate this is a linear program and>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and thank or constabling a programmitrator as a linear programmitrator as a linear program>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <practive context to minimize the cost associated with a nector with a nector with a nector>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <all enc so will stop here>\n",
      "\n",
      "Epoch 82/100\n",
      "774/774 [==============================] - 700s 903ms/step - loss: 0.3543 - val_loss: 0.4291\n",
      "Epoch 83/100\n",
      "774/774 [==============================] - 700s 904ms/step - loss: 0.3536 - val_loss: 0.4287\n",
      "Epoch 84/100\n",
      "774/774 [==============================] - 695s 897ms/step - loss: 0.3531 - val_loss: 0.4278\n",
      "Epoch 85/100\n",
      "774/774 [==============================] - 706s 912ms/step - loss: 0.3525 - val_loss: 0.4273\n",
      "Epoch 86/100\n",
      "774/774 [==============================] - 708s 914ms/step - loss: 0.3520 - val_loss: 0.4261\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interestingly and you can see the prictical matrial matrial matrix>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <you can havelopts it the of so what this we have a large it ten of observe it tend of our what>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <it can of so what this alonger to the same is alonger to the same is alongest tota external>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the total external you hold all hold all right>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <of the for # which is many a linear fashion all innior fashion all right>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <many relinear fashion all right within all right within all right with number of item>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <and within number of iteling a program even formulate this is a linear program and>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and a on constabling a programmitriated at eas a linear programmitrator as a linear program>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that tave conifically conce to minimize the cost associated with a nector with a network>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so instop here or significal contraction calling so will stop here>\n",
      "\n",
      "Epoch 87/100\n",
      "774/774 [==============================] - 699s 902ms/step - loss: 0.3516 - val_loss: 0.4244\n",
      "Epoch 88/100\n",
      "774/774 [==============================] - 706s 912ms/step - loss: 0.3512 - val_loss: 0.4239\n",
      "Epoch 89/100\n",
      "774/774 [==============================] - 739s 954ms/step - loss: 0.3510 - val_loss: 0.4236\n",
      "Epoch 90/100\n",
      "774/774 [==============================] - 709s 915ms/step - loss: 0.3507 - val_loss: 0.4219\n",
      "Epoch 91/100\n",
      "774/774 [==============================] - 723s 933ms/step - loss: 0.3503 - val_loss: 0.4213\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interestingly and you can see the practical matrial matrix of this problem>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <we can have a large is which network this we have a large is which network this>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <it ten of output on longers the tota excelled to to explain along as the tota external>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the total external you hold all right value hold all right>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <processor # which is maning a link all link all link all right>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <varing relinear fashion all right with number of item.>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <and within number of the am even formulate this is a linear program even formulate this>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <and a or constability a callinear programmitted eas a linear programming formunited>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that tave conce to minimize the cost associated with a nector with a nector with a nector>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so it is stop here or significal correction calling so will stop here>\n",
      "\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774/774 [==============================] - 709s 914ms/step - loss: 0.3498 - val_loss: 0.4198\n",
      "Epoch 93/100\n",
      "774/774 [==============================] - 703s 907ms/step - loss: 0.3495 - val_loss: 0.4181\n",
      "Epoch 94/100\n",
      "774/774 [==============================] - 702s 907ms/step - loss: 0.3492 - val_loss: 0.4158\n",
      "Epoch 95/100\n",
      "774/774 [==============================] - 696s 898ms/step - loss: 0.3490 - val_loss: 0.4139\n",
      "Epoch 96/100\n",
      "774/774 [==============================] - 697s 900ms/step - loss: 0.3489 - val_loss: 0.4125\n",
      "target:     <interestingly and you can see the>\n",
      "prediction: <interestingly and you can see the practical mater of this problem we and you can see the problem>\n",
      "\n",
      "target:     <can have a large distribution network>\n",
      "prediction: <we can have a large is which network than the of sument of of something of so what this>\n",
      "\n",
      "target:     <with tens of thousands of hubs what this>\n",
      "prediction: <it can of the same is alongest the of so what the same is alongest the observe tota external>\n",
      "\n",
      "target:     <supply is equal to the total external>\n",
      "prediction: <applies equal to the total external you hold all right general you hold all right>\n",
      "\n",
      "target:     <associated with each link which is>\n",
      "prediction: <and about of # which is manying a link all link all right>\n",
      "\n",
      "target:     <varying in a linear fashion all right>\n",
      "prediction: <are verying a linear fashion all right with number of item.>\n",
      "\n",
      "target:     <with the number of items you can>\n",
      "prediction: <and within number of item even formulate this as a linear program even formulate this as a line>\n",
      "\n",
      "target:     <chain or cost of a distribution network>\n",
      "prediction: <a on constability or can be formunitor can be formunitor can be formunitor can be formunt>\n",
      "\n",
      "target:     <practical context to minimize the cost>\n",
      "prediction: <that you confical context to minimize the cost associated with a network on this>\n",
      "\n",
      "target:     <severe or significant practical>\n",
      "prediction: <so instop saving in paractical comput save here>\n",
      "\n",
      "Epoch 97/100\n",
      "774/774 [==============================] - 701s 905ms/step - loss: 0.3490 - val_loss: 0.4115\n",
      "Epoch 98/100\n",
      "774/774 [==============================] - 702s 906ms/step - loss: 0.3490 - val_loss: 0.4104\n",
      "Epoch 99/100\n",
      "774/774 [==============================] - 696s 898ms/step - loss: 0.3491 - val_loss: 0.4091\n",
      "Epoch 100/100\n",
      "774/774 [==============================] - 702s 906ms/step - loss: 0.3495 - val_loss: 0.4073\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_ds))\n",
    "\n",
    "# The vocabulary to convert predicted indices into characters\n",
    "idx_to_char = vectorizer.get_vocabulary()\n",
    "display_cb = DisplayOutputs(\n",
    "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
    ")  # set the arguments as per vocabulary index for '<' and '>'\n",
    "\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=2,\n",
    "    num_feed_forward=400,\n",
    "    target_maxlen=max_target_len,\n",
    "    num_layers_enc=4,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=34,\n",
    ")\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, label_smoothing=0.1,\n",
    ")\n",
    "\n",
    "learning_rate = CustomSchedule(\n",
    "    init_lr=0.00001,\n",
    "    lr_after_warmup=0.0008,\n",
    "    final_lr=0.00001,\n",
    "    warmup_epochs=25,\n",
    "    decay_epochs=75,\n",
    "    steps_per_epoch=len(ds),\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "EPOCHS = 100\n",
    "checkpoint_filepath = 'checkpoints\\\\weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=False)\n",
    "\n",
    "model.fit(ds, validation_data=val_ds, epochs=1, validation_steps=len(val_ds)/10)\n",
    "model.load_weights('checkpoints\\\\model_original_dataset\\\\weights.22-0.50.hdf5')\n",
    "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb, model_checkpoint_callback], epochs=EPOCHS, validation_steps=len(val_ds)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of transformer_asr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
